package bdf.exercise2.nikokortstrom

// Name: Niko KortstrÃ¶m
// Student number: 014154573

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import org.apache.spark.rdd.RDD

/* Definitions of all other objects, classes, global functions etc go here */

/* Finally we have the Solutions object */
object Solutions {
  
  case class Rating(user_id : String, location : String, age : Int, rating : Int)
  case class Book(isbn : String, title : String, author : String, year : Int, publisher : String, ratings : Iterable[Rating])
  
  def remove_parantheses(value : String) : String = {
    return value.substring(1, value.length()-1)
  }
  
  // parse the values to correct types and remove the parantheses
  def parse_rating(rating : String, user : String) : Rating = {
    val user_id = remove_parantheses(rating.split(";")(0))
    val location = user.split("\"")(3)
    
    var age = -1
    if (user.split(";")(user.split(";").length-1) != "NULL") {
      age = remove_parantheses(user.split(";")(user.split(";").length-1)).toInt
    }
    val rating_number = remove_parantheses(rating.split(";")(2)).toInt
    
    return Rating(user_id, location, age, rating_number)
  }
  
  // parse the values to correct types and remove the parantheses
  def parse_book(book_info : String, ratings : Array[Iterable[Rating]]) : Book = {
    val isbn = remove_parantheses(book_info.split(";")(0))
    val title = book_info.split("\";\"")(1)
    val author = book_info.split("\";\"")(2)
    val year = book_info.split("\";\"")(3).toInt
    val publisher = book_info.split("\";\"")(4)
    
    var rating_iter = Iterable[Rating]()
    if (!ratings.isEmpty) {
      rating_iter = ratings(0)
    }
    
    return Book(isbn, title, author, year, publisher, rating_iter);
  }
  
  def exercise2(rdd : RDD[Book], start_year : Int, end_year : Int) : Long = {
    // get an rdd with only the publications in the correct publication window
    val correct_publications = rdd.filter(book => (book.year >= start_year && book.year <= end_year))
    // get an rdd with only the reviews of these books
    val only_reviews = correct_publications.flatMap(book => book.ratings)
    
    return only_reviews.count
  }
  
  def exercise3(rdd : RDD[Book]) : Array[String] = {
    // get an rdd with only author name as key and iterable of ages of reviewers as value
    val author_reviews = rdd.map(book => (book.author, book.ratings.map(rating => rating.age)))

    // filter out the reviews that don't contain age of the reviewer and filter out authors that have no ratings after this
    val no_ageless_reviews = author_reviews.map(author_review => (author_review._1, author_review._2.filter(age => age != -1))).filter(author_review => !author_review._2.isEmpty)
    
    // format the rdd to contain (author, average_reviewer_age) per element
    val average_ages = no_ageless_reviews.map(no_ageless_review => (no_ageless_review._1, no_ageless_review._2.reduce((age_1, age_2) => age_1 + age_2) / no_ageless_review._2.size))
    
    // return the 20 authors that have highest average reviewer age
    return average_ages.takeOrdered(100)(Ordering[Int].reverse.on(author_rating => author_rating._2)).map(author_rating => author_rating._1)
  }
  
  def exercise4(sc : SparkContext) {
    
  }
  
  def exercise5(sc : SparkContext) {
    
  }
  
  def exercise6(sc : SparkContext) {
    
  }
  
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(getClass.getName).setMaster("local[2]")
    val sc = new SparkContext(conf)
    
    val books = sc.textFile("BX-Books.csv")
    val users = sc.textFile("BX-Users.csv")
    val ratings = sc.textFile("BX-Book-Ratings.csv")
    
    // prepare user rdd for joining
    val user_info = users.keyBy(user => user.split(";")(0))
    // drop the header
    val user_header = user_info.first
    val user_info_without_header = user_info.filter(line => line != user_header)
    
    // prepare rating rdd for joining
    val rating_info = ratings.keyBy(rating => rating.split(";")(0))
    // drop the header
    val rating_header = rating_info.first
    val rating_info_without_header = rating_info.filter(line => line != rating_header)
    
    val user_ratings = rating_info_without_header.join(user_info_without_header)
    // format the tuples to place values in correct places (ISBN, [userID, location, age, rating])
    val formatted_ratings = user_ratings.map(user_rating => (user_rating._2._1.split(";")(1), parse_rating(user_rating._2._1, user_rating._2._2)))
    
    // place the ratings of same book into a list that is in a tuple with isbn as the key
    val book_ratings = formatted_ratings.groupByKey()
    
    // prepare book rdd for joining
    val book_info = books.keyBy(book => book.split(";")(0))
    // drop the header
    val book_header = book_info.first
    val book_info_without_header = book_info.filter(line => line != book_header)
    
    val book_rating_data = book_info_without_header.leftOuterJoin(book_ratings)
    // format the tuples to place values in correct places (isbn, title, author, year, publisher, [Rating])
    val formatted_books = book_rating_data.map(book_rating_info => parse_book(book_rating_info._2._1, book_rating_info._2._2.toArray))
    
    /* Run exercise2 code */
    //val anwer2 = exercise2(formatted_books, 1992, 1998)
    //println("Number of reviews for books published between 1992 and 1998: " + anwer2);
    
    /* Run exercise3 code */
    val answer_3 = exercise3(formatted_books)
    // why on earth is there reviewers in the data with 239 and 228 as age??
    println("Top 20 authors with highest average reviewer age: " + answer_3.mkString(", "))
    
    /* Run exercise4 code */
    exercise4(sc)
    /* Run exercise5 code */
    exercise5(sc)
    /* Run exercise6 code */
    exercise6(sc)
  }
}